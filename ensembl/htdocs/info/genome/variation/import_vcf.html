<html>
<head>
<title>Import VCF script</title>
</head>

<body>

<b>
<a href="index.html">About Ensembl Variation</a> |
<a href="data_description.html">Data Description</a> | 
<a href="predicted_data.html">Predicted Data</a> |
<a href="database.html">Database Description</a> | 
<a href="/info/docs/api/variation/index.html">Perl API</a> |
<a href="vep/index.html">Variant Effect Predictor</a>
</b>
<br /><br />

<h1 id="top">Import VCF script</h1>

<h2 id="contents"> Contents </h2>
<ul>
	<li>
		<a href="#intro">Introduction</a>
	</li>
	<li>
		<a href="#running">Running the script</a>
		<ul>
			<li><a href="#basic">Basic options</a></li>
			<li><a href="#advanced">Advanced options</a></li>
		</ul>
	</li>
	<li>
		<a href="#pop">Populations and frequencies</a>
	</li>
	<li>
		<a href="#tables">Default tables</a>
		<ul>
			<li><a href="#transcript_variation">transcript_variation</a></li>
			<li><a href="#compressed_genotype_region">compressed_genotype_region</a></li>
		</ul>
	</li>
	<li>
		<a href="#scratch">Importing from scratch</a>
		<ul>
			<li><a href="#sql">SQL</a></li>
			<li><a href="#seq_region">seq_region</a></li>
			<li><a href="#attrib">attrib</a></li>
		</ul>
	</li>
	<li>
		<a href="#existing">Importing into an existing database</a>
		<ul>
			<li><a href="#merging">Merging variants</a></li>
			<li><a href="#only_existing">Adding to existing variants</a></li>
		</ul>
	</li>
	<li>
		<a href="#test">Test mode</a>
	</li>
	<li>
		<a href="#recover">Recovering unfinished sessions</a>
		<ul>
			<li><a href="#manual">Manual recovery</a></li>
		</ul>
	</li>
	<li>
		<a href="#fork">Forking</a>
		<ul>
			<li><a href="#recover_fork">Recovery</a></li>
		</ul>
	</li>
</ul>
<hr/>



<h2 id="intro"> Introduction </h2>

<p> The import_vcf.pl script populates an Ensembl database from a VCF (Variant
Call Format) file. A description of the VCF file format can be found on the <a
href="http://www.1000genomes.org/node/101">1000 Genomes project website</a>. The
script can either populate a database from scratch, or add data to an existing
database. </p>

<p> The script can be found in the ensembl-variation CVS module, in the
scripts/import/ sub-directory. </p>

<p> Please note this document assumes a reasonable degree of familiarity with
Ensembl and its internal workings. It may not be suitable for some external
users. For any help with this topic, please email our developer's mailing list
<a href="mailto:dev@ensembl.org">dev@ensembl.org</a></p>

<a href="#top">[Back to top]</a>

<hr/>



<h2 id="running"> Running the script </h2>

<p> The script is run on the command line as follows: </p>

<pre class="code"> perl import_vcf.pl [options] </pre>

<p> where [options] represent a set of flags and options to the script. These
can be listed using the flag --help: </p>

<pre class="code"> perl import_vcf.pl --help </pre>

<p> Options can also be read from a configuration file using --config. </p>

<h3 id="basic"> Basic options </h3>

<table class="ss" style="width:75%;">
	<tr>
		<th>Flag</th>
		<th>Alternate</th>
		<th>Description</th>
	</tr>
	<tr>
		<td><pre>--help</pre></td>
		<td>&nbsp;</td>
		<td>Display help message and quit</td>
	</tr>
	<tr class="bg2">
		<td><pre>--input_file</pre></td>
		<td><pre>-i</pre></td>
		<td>
			Input file name. If not specified, the script will attempt to read
			from STDIN. Input file may be compressed with gzip or bgzip.
		</td>
	</tr>
	<tr>
		<td><pre>--registry [registry file]</pre></td>
		<td>&nbsp;</td>
		<td>
			Registry file containing DB connection details. See <a
			href="/info/docs/api/registry.html">this document</a> for details.
			<i>Required</i>
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--species [species]</pre></td>
		<td>&nbsp;</td>
		<td>
			Species to use for import. Must be either the lowercase,
			underscore-separated latin name, or an alias for the species added
			in the registry file. <i>Default = "homo_sapiens"</i>
		</td>
	</tr>
	<tr>
		<td><pre>--source [source name]</pre></td>
		<td>&nbsp;</td>
		<td>
			Name for source of variants - written to the source table.
			<i>Required</i>
		</td>
	</tr>	
	<tr class="bg2">
		<td><pre>--population [population]</pre></td>
		<td>&nbsp;</td>
		<td>
			Name for population to which all individuals in the file belong.
			<i>Required (or use --panel)</i>
		</td>
	</tr>
</table>



<h3 id="advanced"> Advanced options </h3>

<table class="ss" style="width:75%;">
	<tr>
		<th>Flag</th>
		<th>Alternate</th>
		<th>Description</th>
	</tr>	
	<tr>
		<td><pre>--quiet</pre></td>
		<td><pre>-q</pre></td>
		<td>Suppress status and warning messages</td>
	</tr>
	<tr class="bg2">
		<td><pre>--progress_update [n]</pre></td>
		<td>&nbsp;</td>
		<td>
			Update the progress status after each n variants. This also
			determines how often recovery state is written to disk. To set
			the recovery state frequency to 1 without overloading your
			output with progress messages, add --no_progress <i>Default =
			100</i>
		</td>
	</tr>
	<tr>
		<td><pre>--no_progress</pre></td>
		<td>&nbsp;</td>
		<td>Don't show progress messages. <i>Progress messages shown by
		default</i></td>
	</tr>
	<tr class="bg2">
		<td><pre>--config [filename]</pre></td>
		<td>&nbsp;</td>
		<td>
			Load configuration options from a config file. The config file
			should consist of whitespace-separated pairs of option names and
			settings e.g.: 
<pre>species       mus_musculus
ind_prefix    MYMICE_
registry      ensembl.registry</pre>
		</td>
	</tr>
	<tr>
		<td><pre>--tmpdir [dir]</pre></td>
		<td>&nbsp;</td>
		<td>
			Directory in which to store temporary files. Only used if writing to
			compressed_genotype_region.
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--tmpfile</pre></td>
		<td>&nbsp;</td>
		<td>
			Name for temporary file used by MySQL import. Only used if writing
			to compressed_genotype_region.
		</td>
	</tr>
	<tr>
		<td><pre>--source [source name]</pre></td>
		<td>&nbsp;</td>
		<td>
			Name for source of variants - written to the source table.
			<i>Required</i>
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--source_description [description]</pre></td>
		<td>&nbsp;</td>
		<td>
			Optional text for source description field.
		</td>
	</tr>
	<tr>
		<td><pre>--population [population]</pre></td>
		<td>&nbsp;</td>
		<td>
			Name for population to which all individuals in the file belong.
			<i>Required (or use --panel)</i>
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--panel [panel file]</pre></td>
		<td>&nbsp;</td>
		<td>
			Panel file containing individual population membership. One or
			more of --population or --panel is required. Frequencies are
			calculated for each population specified. Individuals may belong
			to more than one population. Panel file is tab-delimited, with the
			format
<pre>IND1    POP1
IND2    POP1
IND3    POP2
IND4    POP3</pre> <i>Required (or use --population)</i>
		</td>
	</tr>
	<tr>
		<td><pre>--pedigree [pedigree file]</pre></td>
		<td>&nbsp;</td>
		<td>
			Pedigree file containing family relationships and individual
            genders.
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--only_existing</pre></td>
		<td>&nbsp;</td>
		<td>
			Only write to tables when an existing variant is found. Existing
			can be a variation with the same name, or a variant with the same
			location and alleles.
		</td>
	</tr>
	<tr>
		<td><pre>--gmaf [ALL|population]</pre></td>
		<td>&nbsp;</td>
		<td>
			Add global allele frequency data to variation table. "--gmaf ALL"
			uses all individuals in the file; specifying any other population
			name will use the selected population for the GMAF.
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--ind_prefix [prefix]</pre></td>
		<td>&nbsp;</td>
		<td>
			Add prefix to individual names.
		</td>
	</tr>
	<tr>
		<td><pre>--pop_prefix [prefix]</pre></td>
		<td>&nbsp;</td>
		<td>
			Add prefix to population names.
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--var_prefix [prefix]</pre></td>
		<td>&nbsp;</td>
		<td>
			Add prefix to constructed variation names.
		</td>
	</tr>
	<tr>
		<td><pre>--create_name</pre></td>
		<td>&nbsp;</td>
		<td>
			Always create a new variation name i.e. don't use ID column. Names
			are constructed from CHROM and POS fields of VCF.
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--flank [prefix]</pre></td>
		<td>&nbsp;</td>
		<td>
			Size of flanking sequence entry. <i>Default = 200</i>
		</td>
	</tr>
	<tr>
		<td><pre>--gp</pre></td>
		<td>&nbsp;</td>
		<td>
			Use GP tag from INFO column to get coords instead of CHROM and POS
			fields. <i>Script uses CHROM and POS by default</i>
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--tables [list]</pre></td>
		<td>&nbsp;</td>
		<td>
			Comma-separated list of tables to include when writing to DB.
			Overwrites default list.
		</td>
	</tr>
	<tr>
		<td><pre>--add_tables [list]</pre></td>
		<td>&nbsp;</td>
		<td>
			Comma-separated list of tables to add to default list. Use to
            add e.g. compressed_genotype_region, transcript_variation.
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--skip_tables [list]</pre></td>
		<td>&nbsp;</td>
		<td>
			Comma-separated list of tables to exclude when writing to DB. Takes
			precedence over --tables (i.e. any tables named in --tables and
			--skip_tables will be skipped).
		</td>
	</tr>	
	<tr>
		<td><pre>--fork [n]</pre></td>
		<td>&nbsp;</td>
		<td>
			Fork off n simultaneous processes, each dealing with one chromosome
			from the input file. If the number of chromosomes is fewer than the
			number of forks, the input file will be scanned up front and the
			chromosomes sub-divided into regions. Input file must be bgzipped
			and tabix indexed. 10 processes is usually optimal.
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--test [n]</pre></td>
		<td>&nbsp;</td>
		<td>
			Run in test mode on first n lines of file. No database writes
			are done, and any that would be done are output as status
			messages. Cannot be used with --fork. <i>Not used by default</i>
		</td>
	</tr>
	
	<tr>
		<td><pre>--recover</pre></td>
		<td>&nbsp;</td>
		<td>
			Attempt to recover an incomplete session. See <a
			href="#recover">recovery section</a> for more details.
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--recover_pos [pos|chr:pos]</pre></td>
		<td>&nbsp;</td>
		<td>
			Force recover from chromosomal position. Given as either "chr:pos"
			or "pos" alone.
		</td>
	</tr><tr>
		<td><pre>--recover_point [md5]</pre></td>
		<td>&nbsp;</td>
		<td>
			Force recover from a position in the file given by the md5 hash of
			the line content (without newline character).
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--no_recover</pre></td>
		<td>&nbsp;</td>
		<td>
			Disable session recovery - this will result in a slight speed
			increase while running.
		</td>
	</tr>
	
	
	
	<tr>
		<td><pre>--sql</pre></td>
		<td>&nbsp;</td>
		<td>
			Specify SQL file to create tables. Usually found in the
			ensembl-variation CVS checkout, as sql/tables.sql.
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--coord_system [coord system name]</pre></td>
		<td>&nbsp;</td>
		<td>
			If the seq_region table is not populated, by default the script will
			attempt to copy seq_region entries from a Core database specified in
			the registry file. The seq_region entries from the selected
			coord_system will be copied and used. <i>Default = "chromosome"</i>
		</td>
	</tr>
	<tr>
		<td><pre>--backup</pre></td>
		<td>&nbsp;</td>
		<td>
			Backup all affected tables before import.
		</td>
	</tr>
	<tr class="bg2">
		<td><pre>--move</pre></td>
		<td>&nbsp;</td>
		<td>
			Move all affected tables to backed up names and replace with
            empty tables.
		</td>
	</tr>
</table>
  


<p><a href="#top">[Back to top]</a></p>

<hr/>

<h2 id="pop"> Populations and frequencies </h3>

<p> If your VCF file contains individual data, these individuals must belong to
at least one population. You may specify either one population that encompasses
all individual entries in the file (using --population), or a panel file that
contains individual population relationships (using --panel). If you use both,
then individuals will be added to any populations specified in the panel file,
as well as the "global" population specified with --population. </p>

<p> Frequencies for alleles and genotypes are calculated and stored in the
allele and population_genotype tables respectively. Frequencies are calculated
on the fly from the genotypes present in the file. Missing genotypes are not
included in the total count from which the frequency is derived. Frequencies are
calculated for each population specified independently. </p>

<p> Note that a frequency of 0 for an allele is considered valid and will be
entered in the allele table. A frequency of 0 will not be entered for a
population_genotype, however, since the requirements on the population_genotype
table and API are slightly different. </p>

<p> To avoid calculating the frequencies and populating these tables, you may
specify "--skip_tables allele,population_genotype" (or some variant therein to
avoid only one). It may be that you have only a small population for which
population_genotype frequencies are not relevant. You should always (unless you
know what you are doing, or for example are loading on top of an existing DB)
populate the allele table, since it is assumed that each variation entry will
have at least two corresponding alleles with entries in the allele table. </p>

<p> Populations and individuals are only written once - if a population or
individual is found with the same name as the one you have specified, this one
is loaded and used, rather than a new one being created. </p>

<p><a href="#top">[Back to top]</a></p>

<hr/>

<h2 id="tables"> Default tables </h2>

<p> The script by default populates the "basic" set of tables required for a
functioning variation database. For a full description of all tables in the
variation schema, see <a href="variation_schema.html">this page</a>. The
following tables are populated by default: </p>

<ul>
	<li>allele</li>
	<li>allele_code</li>
	<li>compressed_genotype_var</li>
	<li>flanking_sequence</li>
	<li>genotype_code</li>
	<li>individual</li>
	<li>individual_population</li>
	<li>meta_coord</li>
	<li>population</li>
	<li>population_genotype</li>
	<li>sample</li>
	<li>source</li>
	<li>variation</li>
	<li>variation_feature</li>
	<li>variation_synonym</li>
</ul>

<p> You may stop the script populating, for example, the population_genotype
table using "--skip_tables population_genotype", but you should always be aware
of what the consequences of this are before proceeding through a lengthy import!
</p>

<p> There are a set of dependencies built into the script that should prevent
you skipping any tables that are crucial for the use of other tables (for
example, you may not skip allele_code if you want to populate allele). </p>

<p> To see which tables the script will be writing to, do a dry run using --test
and add the --backup flag. </p>

<p> Two tables are skipped by default that may be required. Add them by passing
e.g. "--add_tables transcript_variation". </p>

<h3 id="transcript_variation"> transcript_variation </h3>

<p> The transcript_variation table is not populated by default, as populating
this table depends on calculating the consequence of variants relative to
features in the Core database, which can take a long time. It may also be faster
to do this once the VCF import is finished using the standard
transcript_variation pipeline. </p>

<h3 id="compressed_genotype_region"> compressed_genotype_region </h3>

<p> The compressed_genotype_region table is used to store genotypes in a
per-sample, per-genomic region style. Its contents duplicate that of the
compressed_genotype_var table, but it is used in a different way. The table is
used for resequencing views, and also for the calculation of LD data on the fly
by the API. Currently the designation applies to all samples in the file, so if
you have a file that contains individuals you don't want to be included for
either function then you should split it and import the two separately. </p>

<p> For any data added to the compressed_genotype_region table, you will need to
manually edit the entries in the sample table such that the display field
indicates the relevant data type. If you intend the data to be used for LD
calculation, you should change the <b>population</b> sample entry so that the
display field is "LD". If you intend the data to be used as resequencing data,
you should change the <b>individuals'</b> sample entries so that the display
field is one of "DEFAULT" (switched on the web display by default) or
"DISPLAYABLE" (can be switched on using configure this page). You may also need
to spoof read_coverage entries for any strains you have, unless you have genuine
read coverage data in the read_coverage table. This is as simple as adding an
entry to read_coverage for each strain's sample_id and each chromosome or
seq_region, with a start and end covering the region covered by the resequencing
(this may be the whole chromosome). </p>

<p><a href="#top">[Back to top]</a></p>

<hr/>

<h2 id="scratch"> Importing from scratch </h2>

<p> The script can be used to create a variation database from scratch using a
VCF file. By adding the transcript_variation table as described above, it is
possible to create a website-ready variation DB using this script alone. </p>

<h3 id="sql"> SQL </h3>

<p> You must create your database yourself using MySQL: </p>

<pre>CREATE DATABASE my_var_db;</pre>

<p> Then set up you registry file as described <a
href="/info/docs/api/registry.html">here</a> to point the variation DB entry to
your newly created database (make sure you write the entry as a
Bio::EnsEMBL::Variation::DBSQL::DBAdaptor, as opposed to a
Bio::EnsEMBL::DBSQL::DBAdaptor) . If you want to do transcript_variation, you
should ensure you also have the relevant core DB for your species set up in the
registry file. </p>

<p> The script can create the tables using the table.sql file located in the
ensembl-variation module (located in the sql subdirectory); simply add e.g.
"--sql ../../table.sql" if running the script from the scripts/import
subdirectory. You can also do this manually outside of running the script. </p>

<h3 id="seq_region"> seq_region </h3>

<p> The script requires that the seq_region table be populated so that it can
retrieve seq_region identifiers for each of the chromosomes in your VCF file. If
present, the script will attempt to copy these from the core database. By
default, the script assumes that your data are on the "chromosome" coord_system;
if they are on a different coord_system, you must specify which using
--coord_system. </p>

<h3 id="attrib"> attrib </h3>

<p> In order to calculate variation classes and consequences, the various attrib
tables must be populated. This may be done manually using the script
create_attrib_sql.pl in ensembl-variation/scripts/misc, or automatically by the
VCF import script if --sql is used. If it fails, the script will give a warning
but will not fail; it is possible to fix this after import by populating the
attrib tables and running the transcript_variation/variation class pipeline.
</p>

<p> To be fully website-ready, you should populate transcript_variation either
using the script (with --add_tables transcript_variation), or using the
transcript_variation/variation class pipeline. </p>


<p><a href="#top">[Back to top]</a></p>

<hr/>

<h2 id="existing"> Importing into an existing database </h2>

<p> The script may also be used to add data to an existing database. For
example, the Ensembl Variation team uses the script to add genotypes from the
1000 Genomes project into the existing dbSNP-derived human variation database.
</p>

<h3 id="merging"> Merging variants </h3>

<p> The script will attempt to "merge" variants that occur at the same location.
Insertions and deletions are <b>not</b> considered for merging. Consider two
variants, rs1 and rs2: </p>

<pre class="code">chr1    50     rs1     G     A
chr1    50     rs2     G     T</pre>

<p> The first variant, rs1, will be added as normal, with entries added to
variation, variation_feature etc. The second variant, rs2, will be merged into
rs1. This means in practice that no new variation or variation_feature entry
will be created for rs2. Instead, the variation_feature entry for rs1 will be
updated to include rs2's alleles in its allele_string field; it will change from
G/A to G/A/T. rs2 will be added to variation_synonym as a synonym for rs1 (so,
for example, searching for rs2 on the web would find the page for rs1 with rs2
listed as a synonym). Any linked database entries for rs2, such as those in the
allele table, will be added linked to rs1. </p>

<p> Some more examples of variants that would be merged: </p>

<pre class="code"># same position, same alleles
chr1    100     rs1     C     T
chr1    100     rs2     C     T

# multi-bp substitution
chr1    100     rs1     CG     TT
chr1    100     rs2     CG     TC</pre>

<p> And some examples of variants that would <b>not</b> be merged: </p>

<pre class="code"># same position, one SNP, one deletion
chr1    101     rs1     C     T
chr1    100     rs2     CT    C</pre>

<h3 id="only_existing"> Adding to existing variants </h3>

<p> Use the --only_existing flag to add data only to existing variants; no new
variants will be added by the script. </p>


<p><a href="#top">[Back to top]</a></p>

<hr/>

<h2 id="test"> Test mode </h2>

<p> Test mode can be used to check what database changes will be made by running
the script with a certain set of parameters. The script will print a status
message detailing each SQL process that it will make, without actually executing
the SQL. This can be useful to check the following things: </p>

<ul>
	<li> populations are being added correctly </li>
	<li> frequencies are being calculated correctly </li>
	<li> the expected set of tables is being written to (use with --backup to
	see the list) </li>
</ul>


<p><a href="#top">[Back to top]</a></p>

<hr/>

<h2 id="recover"> Recovering unfinished sessions </h2>

<p> The import VCF script can recover crashed or otherwise unfinished sessions.
To do this, the script stores a status token in a file every n lines (where n is
the parameter set with --progress_update). The token contains a MD5 hash of the
contents of the last line that the script processed. The token is stored in
$HOME/.import_vcf/ with a filename that is another MD5 hash, this time derived
from the composite string of the command-line options passed to the script. </p>

<p> When you set the script to recover with --recover, the script checks for the
recovery token (you must pass the same command-line arguments as the session you
want to recover, obviously with --recover added), then iterates through the
input until it reaches the line whose hash matches the one stored in the
recovery token. Since the script will have stopped somewhere between two
recovery states, it must take some time here to ensure, for example, that
duplicate entries are not added to the allele or genotype tables. </p>

<p> There is a very small time penalty for writing the recovery token every n
lines; it may be disabled for a marginal increase in speed using --no_recover.
</p>

<h3 id="manual"> Manual recovery </h3>

<p> It is also possible to force the script to start from a given position in
the file using --recover_pos; this does not depend on the script finding the
recovery token file. You may also use --recover_point and pass the MD5 of the
line from which you wish to recover. </p>


<p><a href="#top">[Back to top]</a></p>

<hr/>

<h2 id="fork"> Forking </h2>

<p> The speed of import can be vastly increased by running simultaneous
processes. You could do this manually by, for example, running multiple copies
of the script on several different VCF files. </p>

<p> Using --fork takes on this work for you, by splitting your input file into
chunks and running a separate CPU thread on each chunk. Tabix is used to chunk
the file, so you must have the tabix utility in your path to use this. Your
input file must also be bgzipped and tabix indexed. </p>

<p> The script first checks which chromosomes are listed in the index using
"tabix -l"; if the number of chromosomes is less than the number of forks you
have requested (e.g. your VCF may contain data for only one chromosome), the
script will then do an initial scan through the file to determine the range of
positions in the file so that it can be divided into even chunks. This process
will take a few minutes on very large files. </p>

<p> The script will then run each of the forks, with up to the number of forks
you specified running simultaneously. </p>

<h3 id="recover_fork"> Recovery </h3>

<p> Using --fork is still compatible with --recover, as each fork writes its own
recovery token file. It can only be used with the default --recover mode; it
will not work correctly with --recover_pos or --recover_point. </p>


<p><a href="#top">[Back to top]</a></p>


</body>
</html>
